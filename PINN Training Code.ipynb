{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":499712,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":396952,"modelId":415385}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport math\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport time\nimport timeit\nimport math\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom collections import OrderedDict\nfrom tqdm import tqdm\nimport torch\nimport torch.nn as nn\nimport itertools\nfrom scipy.integrate import solve_ivp","metadata":{"id":"4Fi0bbIbfku7","trusted":true,"execution":{"iopub.status.busy":"2025-09-05T17:20:52.064753Z","iopub.execute_input":"2025-09-05T17:20:52.065077Z","iopub.status.idle":"2025-09-05T17:20:54.448691Z","shell.execute_reply.started":"2025-09-05T17:20:52.065044Z","shell.execute_reply":"2025-09-05T17:20:54.448110Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"def GetTBModelDerivatives(y, lamda, mu):\n    N = (lamda/mu) + 1\n    params = [\n    lamda/N,         # lambda (recruitment rate)\n    0.025,     # beta (transmission rate)\n    1,         # delta (differential infectivity)\n    0.3,       # p (fraction that goes directly to infectious)\n    mu,    # mu (natural death rate)\n    0.005,     # k (progression rate from exposed to infectious)\n    0,         # r1 (early treatment effectiveness, not used here)\n    0.8182,    # r2 (treatment rate of infectious)\n    0.02,      # phi (rate from I to L)\n    0.01,      # gamma (reactivation from L to I)\n    0.0227,    # d1 (death rate from I)\n    0.20       # d2 (death rate from L)\n    ]\n    S, E, I, L = y\n    λ, β, δ, p, μ, k, r1, r2, φ, γ, d1, d2 = params\n\n    dSdt = λ - β * S * (I + δ * L) * N - μ * S\n    dEdt = β * (1 - p) * S * (I + δ * L) * N + r2 * I - (μ + k * (1 - r1)) * E\n    dIdt = β * p * S * (I + δ * L) * N + k * (1 - r1) * E + γ * L - (μ + d1 + φ * (1 - r2) + r2) * I\n    dLdt = φ * (1 - r2) * I - (μ + d2 + γ) * L\n\n    if(torch.is_tensor(dSdt)):\n      return dSdt, dEdt, dIdt, dLdt\n    else:\n      return np.array([dSdt, dEdt, dIdt, dLdt])\n\ndef GetTBModelDerivativesForSolveIVP(t, y, lamda, mu):\n    N = (lamda/mu) + 1\n    params = [\n    lamda/N,         # lambda (recruitment rate)\n    0.025,     # beta (transmission rate)\n    1,         # delta (differential infectivity)\n    0.3,       # p (fraction that goes directly to infectious)\n    mu,    # mu (natural death rate)\n    0.005,     # k (progression rate from exposed to infectious)\n    0,         # r1 (early treatment effectiveness, not used here)\n    0.8182,    # r2 (treatment rate of infectious)\n    0.02,      # phi (rate from I to L)\n    0.01,      # gamma (reactivation from L to I)\n    0.0227,    # d1 (death rate from I)\n    0.20       # d2 (death rate from L)\n    ]\n    S, E, I, L = y\n    λ, β, δ, p, μ, k, r1, r2, φ, γ, d1, d2 = params\n\n    dSdt = λ - β * S * (I + δ * L) * N - μ * S\n    dEdt = β * (1 - p) * S * (I + δ * L) * N + r2 * I - (μ + k * (1 - r1)) * E\n    dIdt = β * p * S * (I + δ * L) * N + k * (1 - r1) * E + γ * L - (μ + d1 + φ * (1 - r2) + r2) * I\n    dLdt = φ * (1 - r2) * I - (μ + d2 + γ) * L\n\n    if(torch.is_tensor(dSdt)):\n      return dSdt, dEdt, dIdt, dLdt\n    else:\n      return np.array([dSdt, dEdt, dIdt, dLdt])\n\ndef runge_kutta_4(f, y0, t, lamda, mu):\n    n = len(t)\n    y = np.zeros((n, len(y0)))\n    y[0] = y0\n    for i in range(1, n):\n        h = t[i] - t[i - 1]\n        k1 = f(y[i - 1], lamda, mu)\n        k2 = f(y[i - 1] + h/2 * k1, lamda, mu)\n        k3 = f(y[i - 1] + h/2 * k2, lamda, mu)\n        k4 = f(y[i - 1] + h * k3, lamda, mu)\n        y[i] = y[i - 1] + (h/6) * (k1 + 2*k2 + 2*k3 + k4)\n    return y","metadata":{"id":"ubv4Bp-yZw7y","trusted":true,"execution":{"iopub.status.busy":"2025-09-05T17:20:55.978401Z","iopub.execute_input":"2025-09-05T17:20:55.978778Z","iopub.status.idle":"2025-09-05T17:20:55.995194Z","shell.execute_reply.started":"2025-09-05T17:20:55.978759Z","shell.execute_reply":"2025-09-05T17:20:55.994205Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"def generate_rk_solution(time_true, lamda, mu):\n  time_true = np.array(time_true).flatten()\n  S = lamda / mu\n  N = S + 1\n  y0 = [S/N, 1/N, 0, 0]\n\n  solution = runge_kutta_4(GetTBModelDerivatives, y0, time_true, lamda, mu)\n  S_true, E_true, I_true, L_true = solution.T\n  data = {\n    'Time':time_true,\n    'S': S_true,\n    'E': E_true,\n    'I': I_true,\n    'L': L_true\n}\n\n  # Create DataFrame\n  df = pd.DataFrame(data)\n  return df\n\n\ndef generate_reference_solution(time_true, lamda, mu):\n  time_true = np.array(time_true).flatten()\n  S = lamda / mu\n  N = S + 1\n  y0 = [S/N, 1/N, 0, 0]\n  solution = solve_ivp(GetTBModelDerivativesForSolveIVP, time_true, y0, method=\"BDF\", args = (lamda, mu))\n  data = {\n    'Time':solution.t,\n    'S': solution.y[0],\n    'E': solution.y[1],\n    'I': solution.y[2],\n    'L': solution.y[3]\n  }\n\n  # Create DataFrame\n  df = pd.DataFrame(data)\n  return df\n\ndef CalculateMSEDifferentTimes(S, E, I, L, t_sim):\n    df = generate_reference_solution()\n    t_ref = df['Time'].values.astype(np.float32)\n    S_true = df['S'].values\n    E_true = df['E'].values\n    I_true = df['I'].values\n    L_true = df['L'].values\n\n    # For each reference time, find nearest simulation time\n    indices = np.searchsorted(t_sim, t_ref, side='left')\n    indices = np.clip(indices, 0, len(t_sim)-1)\n\n    # Handle edge cases - check if left or right neighbor is closer\n    for i in range(len(indices)):\n        if indices[i] > 0:\n            left_dist = abs(t_ref[i] - t_sim[indices[i]-1])\n            right_dist = abs(t_ref[i] - t_sim[indices[i]])\n            if left_dist < right_dist:\n                indices[i] -= 1\n\n    S_MSE = np.mean((S_true - S[indices])**2)\n    E_MSE = np.mean((E_true - E[indices])**2)\n    I_MSE = np.mean((I_true - I[indices])**2)\n    L_MSE = np.mean((L_true - L[indices])**2)\n\n    return S_MSE, E_MSE, I_MSE, L_MSE\n\ndef calculate_mse_over_time(t_true, lamda, mu, S_pred, E_pred, I_pred, L_pred, t_pred):\n    \"\"\"\n    Calculate MSE at each time point between predicted and reference values\n    \"\"\"\n    df = generate_reference_solution(t_true, lamda, mu)\n    time_true = df['Time'].values.astype(np.float32)\n    S_true = df['S'].values\n    E_true = df['E'].values\n    I_true = df['I'].values\n    L_true = df['L'].values\n\n    # Initialize MSE arrays\n    mse_S = np.zeros(len(time_true))\n    mse_E = np.zeros(len(time_true))\n    mse_I = np.zeros(len(time_true))\n    mse_L = np.zeros(len(time_true))\n\n    # For each reference time point, find closest predicted time point\n    for i, t_ref in enumerate(time_true):\n        # Find nearest time index in predicted data\n        idx = np.argmin(np.abs(t_pred - t_ref))\n\n        # Calculate squared error at this time point\n        mse_S[i] = (S_true[i] - S_pred[idx])**2\n        mse_E[i] = (E_true[i] - E_pred[idx])**2\n        mse_I[i] = (I_true[i] - I_pred[idx])**2\n        mse_L[i] = (L_true[i] - L_pred[idx])**2\n    return time_true, mse_S, mse_E, mse_I, mse_L","metadata":{"id":"5kB64HLphRjX","trusted":true,"execution":{"iopub.status.busy":"2025-09-05T17:20:59.076580Z","iopub.execute_input":"2025-09-05T17:20:59.076853Z","iopub.status.idle":"2025-09-05T17:20:59.088756Z","shell.execute_reply.started":"2025-09-05T17:20:59.076832Z","shell.execute_reply":"2025-09-05T17:20:59.088014Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":" def PlotResults(time_true, lamda, mu, S_pred, E_pred, I_pred, L_pred, t_pred, title=\"Solution of the ODE System\"):\n  df = generate_reference_solution(time_true,lamda, mu)\n  time_true = df['Time'].values.astype(np.float32)\n  S_true = df['S'].values\n  E_true = df['E'].values\n  I_true = df['I'].values\n  L_true = df['L'].values\n  plt.figure(figsize=(10, 6))\n  plt.subplot(2, 2, 1)\n  plt.plot(t_pred, S_pred, label='S Predicted', color='b', linewidth=2)\n  plt.plot(time_true, S_true, label='S Reference', linestyle='--', color='black')\n  plt.title('S(t) - Susceptible')\n  plt.xlabel('Time (years)')\n  plt.ylabel('S')\n  plt.grid(True)\n  plt.legend()\n\n  plt.subplot(2, 2, 2)\n  plt.plot(t_pred, E_pred, label='E Predicted', color='orange', linewidth=2)\n  plt.plot(time_true, E_true, label='E Reference', linestyle='--', color='black')\n  plt.title('E(t) - Exposed')\n  plt.xlabel('Time (years)')\n  plt.ylabel('E')\n  plt.grid(True)\n  plt.legend()\n\n  plt.subplot(2, 2, 3)\n  plt.plot(t_pred, I_pred, label='I Predicted', color='r', linewidth=2)\n  plt.plot(time_true, I_true, label='I Reference', linestyle='--', color='black')\n  plt.title('I(t) - Infectious')\n  plt.xlabel('Time (years)')\n  plt.ylabel('I')\n  plt.grid(True)\n  plt.legend()\n\n  plt.subplot(2, 2, 4)\n  plt.plot(t_pred, L_pred, label='L Predicted', color='green', linewidth=2)\n  plt.plot(time_true, L_true, label='L Reference', linestyle='--', color='black')\n  plt.title('L(t) - Latent / Out of Sight')\n  plt.xlabel('Time (years)')\n  plt.ylabel('L')\n  plt.grid(True)\n  plt.legend()\n\n  plt.tight_layout()\n  plt.suptitle(title, fontsize=16, y=1.02)\n  plt.show()\n\n\ndef plot_mse_over_time(y_0, t_true, N, S_pred, E_pred, I_pred, L_pred, t_pred, title=\"Mean Square Errors over Time\"):\n    \"\"\"\n    Create a plot showing MSE over time for each compartment\n    \"\"\"\n    # Calculate MSE over time\n    time_points, mse_S, mse_E, mse_I, mse_L = calculate_mse_over_time(y_0, t_true, N, S_pred, E_pred, I_pred, L_pred, t_pred)\n\n    # Create the plot\n    plt.figure(figsize=(10, 6))\n\n    # Plot MSE for each compartment\n    plt.plot(time_points, mse_S, 'o-', label='S', color='orange', linewidth=2, markersize=4)\n    plt.plot(time_points, mse_E, 's-', label='E', color='green', linewidth=2, markersize=4)\n    plt.plot(time_points, mse_I, '^-', label='I', color='blue', linewidth=2, markersize=4)\n    plt.plot(time_points, mse_L, 'd-', label='L', color='red', linewidth=2, markersize=4)\n\n    # Customize the plot\n    plt.xlabel('Time (years)', fontsize=12)\n    plt.ylabel('Mean Square Error', fontsize=12)\n    plt.title(title, fontsize=14)\n    plt.legend(fontsize=10)\n    plt.grid(True, alpha=0.3)\n\n    # Set y-axis to log scale if needed (uncomment if MSE values vary greatly)\n    # plt.yscale('log')\n\n    # Format the plot\n    plt.tight_layout()\n    plt.show()","metadata":{"id":"VTHSQUHZf6R3","trusted":true,"execution":{"iopub.status.busy":"2025-09-05T17:21:04.584158Z","iopub.execute_input":"2025-09-05T17:21:04.584428Z","iopub.status.idle":"2025-09-05T17:21:04.595241Z","shell.execute_reply.started":"2025-09-05T17:21:04.584409Z","shell.execute_reply":"2025-09-05T17:21:04.594617Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"class OptimizedNN(nn.Module):\n    def __init__(\n        self,\n        input_size,\n        output_size,\n        hidden_size=128,  # Reduced from 256\n        num_layers=4,     # Reduced from 6\n        act=torch.nn.Tanh,\n        dropout_rate=0.05  # Reduced dropout\n    ):\n        super(OptimizedNN, self).__init__()\n\n        self.input_size = input_size\n        self.output_size = output_size\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n\n        # Simplified architecture for speed\n        layers = []\n        layers.append(nn.Linear(input_size, hidden_size))\n        layers.append(act())\n\n        for i in range(num_layers):\n            layers.append(nn.Linear(hidden_size, hidden_size))\n            layers.append(act())\n            if i % 2 == 0:  # Add dropout every other layer\n                layers.append(nn.Dropout(dropout_rate))\n\n        layers.append(nn.Linear(hidden_size, output_size))\n\n        self.network = nn.Sequential(*layers)\n\n        # Optimized weight initialization\n        self._initialize_weights()\n\n    def _initialize_weights(self):\n        for m in self.modules():\n            if isinstance(m, nn.Linear):\n                nn.init.xavier_uniform_(m.weight, gain=nn.init.calculate_gain(\"tanh\"))\n                nn.init.constant_(m.bias, 0)\n\n    def forward(self, x):\n        return self.network(x)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-05T17:21:07.567691Z","iopub.execute_input":"2025-09-05T17:21:07.568202Z","iopub.status.idle":"2025-09-05T17:21:07.574398Z","shell.execute_reply.started":"2025-09-05T17:21:07.568178Z","shell.execute_reply":"2025-09-05T17:21:07.573628Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"def check_overflow_basic(values):\n    \"\"\"Basic overflow detection\"\"\"\n    if isinstance(values, (list, tuple)):\n        values = np.array(values)\n    \n    has_inf = np.isinf(values).any()\n    has_nan = np.isnan(values).any()\n    \n    if has_inf:\n        print(\"⚠️  Infinity detected!\")\n        return True\n    if has_nan:\n        print(\"⚠️  NaN detected!\")\n        return True\n    return False","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-05T17:21:10.217610Z","iopub.execute_input":"2025-09-05T17:21:10.218082Z","iopub.status.idle":"2025-09-05T17:21:10.222146Z","shell.execute_reply.started":"2025-09-05T17:21:10.218060Z","shell.execute_reply":"2025-09-05T17:21:10.221436Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset, DataLoader\nimport itertools\nimport numpy as np\n\nclass PINNDataset(Dataset):\n    \"\"\"Custom dataset for PINN training with physics and data points\"\"\"\n    \n    def __init__(self, initial_conditions, true_values, t_physics, ts):\n        self.initial_conditions = initial_conditions\n        self.true_values = true_values\n        self.t_physics = t_physics\n        self.ts = ts\n        \n    def __len__(self):\n        return len(self.initial_conditions)\n    \n    def __getitem__(self, idx):\n        return {\n            'ic': self.initial_conditions[idx],\n            'true_values': self.true_values[idx],\n            'idx': idx\n        }\n\nclass Net:\n    def __init__(self, totalICEpochs, ts):\n        self.device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n\n        #The model arch, modify this or the cell above to modify the arch.\n        self.model = OptimizedNN(\n            input_size=7,\n            output_size=4,\n            hidden_size=128,\n            num_layers=4,\n            act=torch.nn.Tanh,\n            dropout_rate=0\n        ).to(self.device)\n\n        #The initial conditions the model will be trained on\n        self.InitialLamdaValues = np.linspace(1, 11, 10)\n        self.InitialMuValues = np.linspace(0.0101, 0.0227, 20)\n\n        self.TestLamdaValues = np.linspace(1, 5, 4)\n        self.TestMuValues = np.linspace(0.0101, 0.0227, 5)\n\n        self.ts = torch.tensor(ts, dtype=torch.float32).view(-1, 1).to(self.device)\n\n        self.totalICEpochs = totalICEpochs\n\n        self.lambda_physics = 1.0\n        self.lambda_initial = 50.0\n        self.lambda_data = 10.0\n        self.lambda_compartment_sum = 10.0\n\n        self.criterion = torch.nn.MSELoss()\n\n        # DataLoader parameters\n        self.batch_size = 200  # Adjust based on your GPU memory\n        self.num_workers = 0  # Set to 0 for GPU tensors\n\n        self.LBFGS = torch.optim.LBFGS(\n            self.model.parameters(),\n            lr=1.0,\n            max_iter=10000,\n            max_eval=10000,\n            history_size=50,\n            tolerance_grad=1e-7,\n            tolerance_change=1.0 * np.finfo(float).eps,\n            line_search_fn=\"strong_wolfe\",   # better numerical stability\n        )\n\n        self.huber_loss = torch.nn.SmoothL1Loss()\n\n        self.adam = torch.optim.AdamW(\n                    self.model.parameters(), \n                    lr=0.0001,           # Start lower than Adam\n                    weight_decay=1e-4,  # Typical range: 1e-5 to 1e-3\n                    betas=(0.9, 0.999)\n                )\n        self.scheduler = torch.optim.lr_scheduler.ExponentialLR(\n            self.adam, \n            gamma=0.99  # Decay factor (lr *= gamma each step)\n        )\n\n        self.h = 0.1\n        t = torch.arange(0, 1 + self.h, self.h)\n\n        # Reduce physics time points to save memory\n        self.t_physics = torch.cat([\n            torch.linspace(0, 30, 101),      # Reduced from 201\n        ], dim=0).view(-1, 1).to(self.device).requires_grad_(True)\n\n        self.t_initial = torch.tensor([0.0], requires_grad=True).view(-1, 1).to(self.device)\n\n        self.lossData = 1000\n        self.loss_history = []\n        \n        # Will be set during Train()\n        self.dataloader = None\n\n    # A slightly modified Forward method\n    def Forward(self, fullInput, t_grad, is_batched=False):\n        outputs = self.model(fullInput)\n        S = outputs[:, 0:1]\n        E = outputs[:, 1:2]\n        I = outputs[:, 2:3]\n        L = outputs[:, 3:4]\n        \n        # The autograd call remains the same, as it operates on the entire tensor\n        Sprime = torch.autograd.grad(S, t_grad, grad_outputs=torch.ones_like(S), create_graph=True)[0]\n        Eprime = torch.autograd.grad(E, t_grad, grad_outputs=torch.ones_like(E), create_graph=True)[0]\n        Iprime = torch.autograd.grad(I, t_grad, grad_outputs=torch.ones_like(I), create_graph=True)[0]\n        Lprime = torch.autograd.grad(L, t_grad, grad_outputs=torch.ones_like(L), create_graph=True)[0]\n        \n        return S, E, I, L, Sprime, Eprime, Iprime, Lprime\n\n    def compute_batch_loss(self, batch_ics, batch_true_values):\n        \"\"\"Compute loss for a single batch (balanced with relative residuals + Pearson correlation)\"\"\"\n        batch_size = batch_ics.shape[0]\n    \n        # --- Physics Loss ---\n        t_physics_expanded = self.t_physics.repeat(batch_size, 1)\n        ics_expanded_physics = batch_ics.repeat_interleave(self.t_physics.shape[0], dim=0)\n        physics_input = torch.cat([t_physics_expanded, ics_expanded_physics], dim=1)\n    \n        # Forward pass for physics\n        S, E, I, L, S_prime, E_prime, I_prime, L_prime = self.Forward(\n            physics_input, t_physics_expanded, is_batched=True\n        )\n    \n        # True derivatives from ODE\n        S_prime_true, E_prime_true, I_prime_true, L_prime_true = GetTBModelDerivatives(\n            [S, E, I, L],\n            physics_input[:, 1],  # lambda\n            physics_input[:, 2]   # mu\n        )\n    \n        # Relative residual loss (scale-invariant)\n        def scaled_mse(pred, true, eps=1e-8):\n            scale = torch.mean(torch.abs(true)) + eps\n            return torch.mean(((pred - true) / scale) ** 2)\n        \n        loss_ode_S = self.criterion(S_prime, S_prime_true)\n        loss_ode_E = self.criterion(E_prime, E_prime_true)\n        loss_ode_I = self.criterion(I_prime, I_prime_true)\n        loss_ode_L = self.criterion(L_prime, L_prime_true)\n        \n        physics_loss = loss_ode_S + loss_ode_E + loss_ode_I + loss_ode_L\n    \n        # Compartment sum constraint\n        total_sum_of_compartments = S + E + I + L\n        true_sum = torch.ones_like(S)\n        compartment_sum_loss = self.criterion(total_sum_of_compartments, true_sum)\n    \n        # --- Data Loss ---\n        ts_expanded = self.ts.repeat(batch_size, 1)\n        ics_expanded_data = batch_ics.repeat_interleave(self.ts.shape[0], dim=0)\n        data_input = torch.cat([ts_expanded, ics_expanded_data], dim=1)\n    \n        predicted_data = self.model(data_input)\n        batch_true_values_reshaped = torch.cat([tv for tv in batch_true_values], dim=0)\n    \n        # Normalized MSE (scale-invariant)\n        def normalized_mse(pred, true, eps=1e-8):\n            denom = torch.mean(torch.abs(true), dim=0, keepdim=True) + eps\n            return torch.mean(((pred - true) / denom) ** 2)\n    \n        # Pearson correlation loss\n        def pearson_correlation_loss(pred, true):\n            pred_flat = pred.reshape(-1)\n            true_flat = true.reshape(-1)\n            pred_centered = pred_flat - pred_flat.mean()\n            true_centered = true_flat - true_flat.mean()\n            numerator = torch.sum(pred_centered * true_centered)\n            denom = torch.sqrt(torch.sum(pred_centered**2)) * torch.sqrt(torch.sum(true_centered**2)) + 1e-8\n            corr = numerator / denom\n            return (1.0 - corr) ** 2\n    \n        mse_loss = normalized_mse(predicted_data, batch_true_values_reshaped)\n        corr_loss = pearson_correlation_loss(predicted_data, batch_true_values_reshaped)\n        data_loss = mse_loss + corr_loss\n    \n        # --- Total Loss ---\n        total_loss = (\n            self.lambda_physics * physics_loss +\n            self.lambda_data * data_loss +\n            self.lambda_compartment_sum * compartment_sum_loss\n        )\n    \n        return total_loss\n\n\n\n    def compute_loss_with_dataloader(self):\n        \"\"\"Compute total loss using DataLoader\"\"\"\n        total_loss = 0.0\n        num_batches = 0\n        \n        for batch in self.dataloader:\n            batch_ics = batch['ic'].to(self.device)\n            batch_true_values = batch['true_values'].to(self.device)\n            \n            batch_loss = self.compute_batch_loss(batch_ics, batch_true_values)\n            total_loss += batch_loss\n            num_batches += 1\n        \n        # Average loss across batches\n        return total_loss / num_batches\n\n    def Train(self):\n        # Prepare data\n        iterables = [self.InitialLamdaValues, self.InitialMuValues]\n        allIC_candidates = [list(x) for x in itertools.product(*iterables)]\n\n        test_iterables = [self.TestLamdaValues, self.TestMuValues]\n        allIC_test_candidates = [list(x) for x in itertools.product(*test_iterables)]\n        allIC = []\n        allTrueValues = []\n        \n        print(f\"Generating reference solutions for {len(allIC_candidates)} initial conditions...\")\n        for currentInitialValue in allIC_candidates:\n            df = generate_rk_solution(self.ts.detach().cpu().numpy().flatten(), currentInitialValue[0], currentInitialValue[1])\n            t_ref = df['Time'].values.astype(np.float32)\n            S_true = df['S'].values\n            E_true = df['E'].values\n            I_true = df['I'].values\n            L_true = df['L'].values\n            currentTrueValues = np.array([S_true, E_true, I_true, L_true])\n            if(check_overflow_basic(currentTrueValues) is False):\n                currentTrueValues = torch.transpose(torch.tensor(currentTrueValues, dtype=torch.float32), 0, 1)\n                allTrueValues.append(currentTrueValues)\n                S_0 = currentInitialValue[0] / (currentInitialValue[0]+currentInitialValue[1])\n                E_0 = 1 / ((currentInitialValue[0] / currentInitialValue[1]) + 1)\n                I_0 = 0\n                L_0 = 0\n                allIC.append(currentInitialValue + [S_0, E_0, I_0, L_0])\n\n        print(f\"Generating reference solutions for {len(allIC_test_candidates)} test initial conditions...\")\n        testTrueValues = []\n        allIC_test = []\n        for currentInitialValue in allIC_test_candidates:\n            df = generate_rk_solution(self.ts.detach().cpu().numpy().flatten(), currentInitialValue[0], currentInitialValue[1])\n            t_ref = df['Time'].values.astype(np.float32)\n            S_true = df['S'].values\n            E_true = df['E'].values\n            I_true = df['I'].values\n            L_true = df['L'].values\n            currentTrueValues = np.array([S_true, E_true, I_true, L_true])\n            if(check_overflow_basic(currentTrueValues) is False):\n                currentTrueValues = torch.transpose(torch.tensor(currentTrueValues, dtype=torch.float32), 0, 1)\n                testTrueValues.append(currentTrueValues)\n                S_0 = currentInitialValue[0] / (currentInitialValue[0]+currentInitialValue[1])\n                E_0 = 1 / ((currentInitialValue[0] / currentInitialValue[1]) + 1)\n                I_0 = 0\n                L_0 = 0\n                allIC_test.append(currentInitialValue + [S_0, E_0, I_0, L_0])\n        self.testTrueValues = testTrueValues\n        self.test_ics_tensor = torch.tensor(allIC_test, dtype=torch.float32)\n        # Convert to tensors\n        all_ics_tensor = torch.tensor(allIC, dtype=torch.float32)\n        # Create dataset and dataloader\n        dataset = PINNDataset(all_ics_tensor, allTrueValues, self.t_physics, self.ts)\n        self.dataloader = DataLoader(\n            dataset, \n            batch_size=self.batch_size, \n            shuffle=True, \n            num_workers=self.num_workers,\n            pin_memory=True if self.device.type == 'cuda' else False\n        )\n        \n        print(f\"Training with {len(allIC)} initial conditions\")\n        print(f\"Batch size: {self.batch_size}, Number of batches: {len(self.dataloader)}\")\n        print(\"Starting new training!\")\n\n        best_loss = 100000\n        for epoch in range(self.totalICEpochs):\n            # Update lambda values based on epoch\n            if epoch < 400:\n                self.lambda_physics = 100\n                self.lambda_data = 10\n            elif epoch < 800:\n                self.lambda_physics = 70\n                self.lambda_data = 30\n            elif epoch < 1200:\n                self.lambda_physics = 70\n                self.lambda_data = 70\n            else:\n                self.lambda_physics = 100.0\n                self.lambda_data = 400.0\n            \n            epoch_loss = 0.0\n            num_batches = 0\n            \n            # Training loop with DataLoader\n            for batch in self.dataloader:\n                self.adam.zero_grad()\n                \n                batch_ics = batch['ic'].to(self.device)\n                batch_true_values = batch['true_values'].to(self.device)\n                \n                batch_loss = self.compute_batch_loss(batch_ics, batch_true_values)\n                batch_loss.backward()\n                \n                # Optional gradient clipping\n                # torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n                \n                self.adam.step()\n                \n                epoch_loss += batch_loss.item()\n                num_batches += 1\n            \n            # Average loss for the epoch\n            avg_epoch_loss = epoch_loss / num_batches\n            self.loss_history.append(avg_epoch_loss)\n\n            #print(\"Epoch:\", epoch, \"done!\")\n            \n            if (epoch + 1) % 1000 == 0:\n                self.scheduler.step()\n                print(f\"Adam Epoch [{epoch+1}], Loss: {avg_epoch_loss:.8f}, LR: {self.scheduler.get_last_lr()[0]:.6f}\")\n                current_loss = self.EvaluateModel()\n                print(\"Validation Loss:\", current_loss)\n                if(current_loss < best_loss):\n                    print(\"New best model found!\")\n                    torch.save(self, f'checkpoint_epoch_{epoch+1}.pth')\n                    best_loss = current_loss\n            \n            # Clear cache periodically\n            if (epoch + 1) % 100 == 0:\n                torch.cuda.empty_cache()\n\n        # LBFGS fine-tuning\n        print(\"Starting LBFGS fine-tuning...\")\n        def closure():\n            self.LBFGS.zero_grad()\n            total_loss = self.compute_loss_with_dataloader()\n            self.loss_history.append(total_loss.item())\n            total_loss.backward()\n            return total_loss\n\n        self.LBFGS.step(closure)\n        final_loss = self.compute_loss_with_dataloader()\n        print(f\"\\nTraining finished. Final Loss: {final_loss.item():.6f}\")\n        if(final_loss.item() < best_loss):\n            print(\"New best model found!\")\n            torch.save(self, f'checkpoint_epoch_{epoch+1}.pth')\n    \n    def PredictSingle(self, t, y0):\n        self.model.eval()\n        with torch.no_grad():\n            inputTensor = torch.tensor([np.hstack((t, y0))], device=self.device, dtype=torch.float32)\n            output = self.model(inputTensor)\n            return output.detach().cpu().numpy().flatten()\n\n    def PredictArray(self, t, lamda, mu, S0, E0, I0, L0):\n        self.model.eval()\n        with torch.no_grad():\n            t = torch.tensor([t], dtype=torch.float32, device=self.device)\n            # Reshape t to be [n_points, 1] if it isn't already\n            t_reshaped = t.reshape(-1, 1)\n\n            # Create tensor of initial values repeated for each time point\n            n_points = t_reshaped.shape[0]\n            initial_values = torch.tensor([[lamda, mu, S0, E0, I0, L0]], dtype=torch.float32, device=self.device).repeat(n_points, 1)\n            # Concatenate time points with initial values\n            fullInput = torch.cat([t_reshaped, initial_values], dim=1)\n            # Move to device and set requires_grad\n            fullInput = fullInput.to(self.device).requires_grad_(True)\n            inputTensor = fullInput\n            output = self.model(inputTensor)\n            return output.squeeze().detach().cpu().numpy()\n\n    def EvaluateModel(self):\n        total_loss = 0.0\n        n_tests = len(self.test_ics_tensor)\n    \n        # Normalized MSE (scale-invariant)\n        def normalized_mse(pred, true, eps=1e-8):\n            denom = torch.mean(torch.abs(true), dim=0, keepdim=True) + eps\n            return torch.mean(((pred - true) / denom) ** 2)\n    \n        # Pearson correlation loss\n        def pearson_correlation_loss(pred, true):\n            pred_flat = pred.reshape(-1)\n            true_flat = true.reshape(-1)\n            pred_centered = pred_flat - pred_flat.mean()\n            true_centered = true_flat - true_flat.mean()\n            numerator = torch.sum(pred_centered * true_centered)\n            denom = torch.sqrt(torch.sum(pred_centered**2)) * torch.sqrt(torch.sum(true_centered**2)) + 1e-8\n            corr = numerator / denom\n            return (1.0 - corr) ** 2\n    \n        for i in range(n_tests):\n            IC = self.test_ics_tensor[i]\n    \n            # predict full trajectory for this IC\n            currentSolution = self.PredictArray(\n                self.ts.detach().cpu().numpy().flatten(),\n                IC[0], IC[1], IC[2], IC[3], IC[4], IC[5]\n            )\n            currentSolution = torch.tensor(currentSolution, dtype=torch.float32, device=self.device)\n    \n            trueSolution = self.testTrueValues[i].to(self.device)\n    \n            # Data loss = NMSE + Pearson\n            mse_loss = normalized_mse(currentSolution, trueSolution)\n            corr_loss = pearson_correlation_loss(currentSolution, trueSolution)\n            currentLoss = mse_loss + corr_loss\n    \n            total_loss += currentLoss.item()\n    \n        return total_loss / n_tests\n","metadata":{"id":"QLxGdfXQtwFd","trusted":true,"execution":{"iopub.status.busy":"2025-09-05T17:21:12.286290Z","iopub.execute_input":"2025-09-05T17:21:12.286796Z","iopub.status.idle":"2025-09-05T17:21:12.323354Z","shell.execute_reply.started":"2025-09-05T17:21:12.286771Z","shell.execute_reply":"2025-09-05T17:21:12.322729Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"time_domain = np.linspace(0, 20, 41)\nModel = Net(30000, time_domain) \nModel.Train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-05T17:39:16.195497Z","iopub.execute_input":"2025-09-05T17:39:16.196033Z"}},"outputs":[{"name":"stdout","text":"Generating reference solutions for 200 initial conditions...\nGenerating reference solutions for 20 test initial conditions...\nTraining with 200 initial conditions\nBatch size: 200, Number of batches: 1\nStarting new training!\nAdam Epoch [1000], Loss: 6485.29150391, LR: 0.000099\nValidation Loss: 749629.3912334442\nAdam Epoch [2000], Loss: 6506.93701172, LR: 0.000098\nValidation Loss: 67084.49577615262\nNew best model found!\nAdam Epoch [3000], Loss: 3502.27929688, LR: 0.000097\nValidation Loss: 33679.44418091774\nNew best model found!\nAdam Epoch [4000], Loss: 2361.54003906, LR: 0.000096\nValidation Loss: 24600.313498020172\nNew best model found!\nAdam Epoch [5000], Loss: 1776.42687988, LR: 0.000095\nValidation Loss: 18733.114101088046\nNew best model found!\n","output_type":"stream"}],"execution_count":null},{"cell_type":"raw","source":"","metadata":{"execution":{"iopub.status.busy":"2025-09-05T17:21:18.528324Z","iopub.execute_input":"2025-09-05T17:21:18.529022Z"}}},{"cell_type":"code","source":"Model_Best = torch.load(\"checkpoint_epoch_24000.pth\", weights_only=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"time_domain = np.linspace(0, 20, 41)\nlamda = 4\nmu = 0.02\nmodel_solutions = Model.PredictArray(time_domain, lamda, mu)\nS_pred = model_solutions[:,0]# * N\nE_pred = model_solutions[:,1]#* N\nI_pred = model_solutions[:,2]#* N\nL_pred = model_solutions[:,3]#* N\nPlotResults([0, 20], lamda, mu, S_pred, E_pred, I_pred, L_pred, time_domain)\nplot_mse_over_time([0, 20], lamda, mu, S_pred, E_pred, I_pred, L_pred, time_domain)","metadata":{"id":"4tHbOxSB0vFA","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"criterion = torch.nn.MSELoss()\ncompartment_losses = {'S': 0, 'E': 0, 'I': 0, 'L': 0}\n\nfor i in range(len(Model_Best.test_ics_tensor)):\n    IC = Model_Best.test_ics_tensor[i]\n    predicted = Model_Best.PredictArray(Model_Best.ts.detach().cpu().numpy().flatten(), IC[0], IC[1])\n    predicted = torch.tensor(predicted, dtype=torch.float32)\n    true_values = Model_Best.testTrueValues[i]\n    \n    compartment_losses['S'] += criterion(predicted[:, 0], true_values[:, 0]).item()\n    compartment_losses['E'] += criterion(predicted[:, 1], true_values[:, 1]).item()\n    compartment_losses['I'] += criterion(predicted[:, 2], true_values[:, 2]).item()\n    compartment_losses['L'] += criterion(predicted[:, 3], true_values[:, 3]).item()\n\n# Average over all test cases\nfor key in compartment_losses:\n    compartment_losses[key] /= len(Model_Best.test_ics_tensor)\n    print(f\"MSE {key}: {compartment_losses[key]:.12f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"torch.save(Model, \"workingModel.pth\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"y0 = [90/N, 1/N, 0, 0]\nmodel_solutions = Model.PredictArray(time_domain, y0)\nS_pred = model_solutions[:,0]*N\nE_pred = model_solutions[:,1]*N\nI_pred = model_solutions[:,2]*N\nL_pred = model_solutions[:,3]*N\nPlotResults(y0, time_domain, S_pred, E_pred, I_pred, L_pred, time_domain)\nplot_mse_over_time(y0, time_domain, S_pred, E_pred, I_pred, L_pred, time_domain)","metadata":{"id":"K3Cgoao0S_7Q","trusted":true},"outputs":[],"execution_count":null}]}